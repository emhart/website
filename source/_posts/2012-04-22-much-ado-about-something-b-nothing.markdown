---
layout: post
title: "Much ado about A). something B). nothing"
date: 2012-04-22
comments: false
---

<div class='post'>
I've been following with interest Bob O'Hara's blog <a href="http://blogs.nature.com/boboh/2012/03/23/the-problems-of-interpreting-data">post</a> over at his blog, <a href="http://blogs.nature.com/boboh/">"Deep thoughts and&nbsp;</a><br /><a href="http://blogs.nature.com/boboh/">silliness"</a>.  In his post he is highly critical of a piece that came out in <i>Science</i> earlier this year by <a href="http://www.escet.urjc.es/biodiversos/espa/personal/fernando/index_en.html">Fernando Maestre</a>, <a href="http://www.uvm.edu/~ngotelli/homepage.html">Nick Gotelli</a> and many others (full disclosure I was Nick's graduate student up until last year and have known Fernando for several years).  Bob's problems seemed to be two fold, stemming from an assertion by Fernando that "significant relationships would indicate potentially strong effects of richness on multifunctionality".  <br /><table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: right; text-align: right;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-e-h_RjAioSg/T5NgZkZMvYI/AAAAAAAADKE/DJaX9OwWrnI/s1600/blogplot1.png" imageanchor="1" style="clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;"><img border="0" height="200" src="http://4.bp.blogspot.com/-e-h_RjAioSg/T5NgZkZMvYI/AAAAAAAADKE/DJaX9OwWrnI/s200/blogplot1.png" width="200" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Red lines from errors in x, blue from errors in y as errors<br />in x increase, slopes tend to 0. As they increase in y,<br />slopes vary randomly. &nbsp;The true slope is a thick dashed<br />black line</td></tr></tbody></table>Bob feels this is "utter rubbish", and has two reasons for the caustic remark.  His first problem is that there are no measures of within variable variability (an awkward phrase I know).  Doing a bit of simulation he shows what has been known for a long time, that in a regression errors in <i>y</i> produce symmetric unbiased estimates of the intercept and slope and whereas errors in <i>x</i> produce biased estimates.<br />This is of course no surprise, it has certainly been published in ecology before, a quick search brought me to this <a href="http://www.esajournals.org/doi/abs/10.2307/1937451">paper in 1994 by Steve Carpenter in <i>Ecology</i></a>, and I'd bet it come up before that.  I would argue that the fundamental importance of Fernando's work is not the magnitude of the slope, but that the relationship is present and holds up across a huge observational data set.  The true measure of interest is how much of the variation can be explained by species richness, and how does error-in-variable affect that?  So I quickly did Bob's simulations as best I could inferring his code from the text.   <br /><br /><table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em; text-align: left;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-MNNvPgZTw_c/T5RzNp0SeAI/AAAAAAAADLg/37JsoU6eRg8/s1600/slopeplot.png" imageanchor="1" style="clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;"><img border="0" height="200" src="http://3.bp.blogspot.com/-MNNvPgZTw_c/T5RzNp0SeAI/AAAAAAAADLg/37JsoU6eRg8/s200/slopeplot.png" width="200" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Errors in y produce almost no bias in slope <br />&nbsp;but errors in x always underestimate the<br />slope. &nbsp;The black line is the true value.</td></tr><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-J9ktjx-HdK0/T5R4qNRa2YI/AAAAAAAADLw/4WR8KYFj39c/s1600/Rsqplot.png" imageanchor="1" style="clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;"><img border="0" height="200" src="http://2.bp.blogspot.com/-J9ktjx-HdK0/T5R4qNRa2YI/AAAAAAAADLw/4WR8KYFj39c/s200/Rsqplot.png" width="200" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Errors in x and y produce<br />similar bias in estimates<br />of R<sup>2</sup></td></tr></tbody></table> <b>A few methods</b>. <br /><br />You can see my <a href="https://github.com/emhart/Misc_Func/blob/master/slopeBias.R">full code over at my GitHub page</a>.  I first created a multiple regression with four variables, all equal in their effects.  I chose one variable at random and imagined that I only measured my response and that one variable, yielding an R<sup>2</sup> of ~%25.   I then I ran two simulations, one with measurement error in <i>y</i> and another with error in <i>x</i>, such that the correlation between the true value and "corrupted" value of <i>x</i> and <i>y</i> was between 0 to 1, the same as Bob.  Its clear that errors measurement error in <i>x</i> has a different bias than error in <i>y.</i>Bob's problem, as I understand it, is that when measuring species richness surely there is measurement error in that variable.  This error will necessarily bias the slope.  I would argue that Fernando's central point was that it explained some of the variance.  So when I ran my simulation, I asked how do errors in variable affect values of R<sup>2</sup>?  Plotting that reveals a non-linear increase for both error types as error decreases.  So at worst they are underestimating the amount of variance in multifunctionality (which is quite low at 4%).  Therefore if the quantity of interest is the slope, then errors in variables matter, but if the quantity of interest is R<sup>2</sup> then the location of the error is less relevant. I've already made my case that its the latter so Bob's technical objections seem moot. <br /><br /><b>So what does it all mean</b><br /><br />The next objection he raises is of a much more general nature about observational studies and I agree with it in part. One of his last sentences sums it up.    <blockquote class="tr_bq">If we estimate an effect of species richness (say), it could be that the true effect of biodiversity is larger and we are only measuring an indicator, or it could be that there is no effect, but both ecosystem function and species richness are affected by the same abiotic drivers.</blockquote>The former point is quite astute.  Most ecological studies do measure some dependent variable of interest (multifunctionality in this case), and several independent variables that may be indicative of some process / mechanism but not the actual mechanism.  Bob's example is that an investigator may measure winter temperature and water bird abundance, but the true mechanism may pond ice out date.  His point: mechanisms are complicated and observational studies often miss these nuances.  The second point falls into my bin of "pointless criticisms of *study type*".  Yes, observational studies can never show causality with with complete confidence, but does that mean we should dismiss them all together?  In ecology almost any study falls somewhere in the range between obeservational and microcosm.  Each experiment type, from large scale macorecology studies to mesocosms to microcosms are up for some sort of criticism along these lines.  For instance microcosms are often criticized as being unrealistic because of their small scale, should we stop doing them?(if you say yes <a href="http://oikosjournal.wordpress.com/2011/06/10/objections-to-microcosms-in-ecology-and-their-answers/">Jeremy Fox has an earful for you</a>).   <br><br>I think that most ecologists would agree that hypotheses within our field need a robust series of results from different experiment types.  We need good theory, good mathematical underpinnings, but also evidence from large scale surveys, manipulative field experiments, microcosms etc... to have strong support for a given hypothesis.  In the end Bob's problem with errors in variables seems irrelevant given that the magnitude of the slope was not the quantity of interest. His points about how mechanisms are complicated is valid, but he follows it up with essentially dismissing all observational studies as incapable of demonstrating causality. Is Fernando's paper the alpha and omega on biodiversity and ecosystem function?  Of course not, but is it "utter rubbish" as Bob says?  I'd hardly say the paper warrants the vitriol.</div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Bob O'H</div>
<div class='content'>
The indirect effects of drivers are pretty weak. But the correlation between N and C functions is huge (and the covariate effects all very simar), which suggests to me a common underlying. cause. The response of P has a lower correlation, but it&#39;s still fairly strong. So I suspect there&#39;s an unmeasured driver affecting multifunctionality, but not perennial plant species richness.</div>
</div>
<div class='comment'>
<div class='author'>jebyrnes</div>
<div class='content'>
So, in addition to the standardized regression coefficient stuff I talked about on Bob&#39;s blog, I also decided to go in and use SEM to test the assumption that Multifunction &amp; Diversity may merely be the product of a common suite of drivers. The code is at <a href="https://gist.github.com/2474422" rel="nofollow">this gist</a>.<br /><br /> There&#39;s one thing that requires some careful care in this analysis - namely, are we making the assumption that a) Richness and Multifunction are driven by the same OBSERVED drivers only, and do not covary beyond that or b) that there are additional unmeasured drivers beyond what is in this data set.The reason I ask this is, if b, then we cannot test the assumption that richness causally affects multifunction or not.  A model where Richness -&gt; Multifunction versus a model where Richness &lt;-&gt; Multifunction (i.e., there is some external driver beyond the measured covariates that affect both of them) But, assuming case a - and there is reason to argue for it - we would reject this as what is being observed in the system (LR Chisq = 10.802, df=1, p=0.001014) Indeed, there is even some evidence for indirect effect of environmental drivers via diversity - PCA3, for example, affect diversity directly, but not multifunction.  <br /><br />The other nice thing about this framework is that if the measurement error of SR is known, that could be folded into this SEM to get a more unbiased result.<br /><br />But, again, I caution that this interpretation is based on my understanding of the model from the paper, and something seems a little off in some AIC values. Also that I am making assumption a, which may or may not be valid.  We&#39;d need something that unambiguously affected one or the other and not both in order to build two non-equivalent models if b is the case.<br /><br />There&#39;s some neat stuff again here, though!</div>
</div>
<div class='comment'>
<div class='author'>Bob O'Hara</div>
<div class='content'>
Thanks for the interest. A couple of comments:<br /><br />1. The effects of errors in variables on parameter estimates and R2 are related: if we have a regression with a single parameter the variance due to the parameter <i>β</i> is <i>β</i>2 Var(X). Hence, if the coefficient is attenuated towards 0 by a factor <i>c</i>, <i>R</i>2 is reduced by  <i>c</i>2. My point was that errors in variables tend to reduce the explanatory power, so I think that still holds. (BTW, it gets more complicated with &gt;1 explanatory variable: it&#39;s possible to over-estimate one effect)<br />2. I certainly wouldn&#39;t describe the paper as &quot;utter rubbish&quot;, only that one sentence.I don&#39;t have a problem with people doing observational studies (I&#39;ve worked on that sort of data myself). I was just trying to point out some of the subtler problems with observational data, and was using the paper as a jumping off point.</div>
</div>
<div class='comment'>
<div class='author'>Ted Hart</div>
<div class='content'>
Thanks for the reply Mick.  I don&#39;t want to haggle over what Bob meant exactly, but I took issue because I think we&#39;d all be better served with polite discourse.  For instance he might have reworded his statement as: &quot;.  But this assertion is unfounded for reasons xyz&quot;.  I realize this makes for drab reading, but myself and other commenters have taken issue with this, and I think in the end it just undermines his point because we all get caught up in its pejorative nature.   Point 2 though I think is very valid, and as you mentioned in the comments its a problem that can be addressed with hierarchical models.   I think aside from the technical hurdles it poses for many, people still think of hierarchical and Bayesian as interchangeable and some feel that&#39;s a dirty word.  It also requires repeated measures, and for some studies this data isn&#39;t feasible to collect, but I whole heartedly agree there&#39;s a certain poverty of intellect to know a problem exists, and a solution exists and just not take advantage of it because its hard.</div>
</div>
<div class='comment'>
<div class='author'>Michael McCarthy</div>
<div class='content'>
From my reading of Bob&#39;s post, I think this and some other responses I have read miss his main point. This is what I got from Bob&#39;s post:<br /><br />1) Bob said that the statement &quot;because we did not experimentally control for other abiotic and biotic<br /> factors that are known to affect ecosystem functioning, significant <br />relationships would indicate potentially strong effects of richness on <br />multifunctionality&quot;, was &quot;utter rubbish&quot;. He claimed this because correlation does not equal causation (and I would add because statistical significance does not equate with importance). &quot;Utter rubbish&quot; might overstate things, but I don&#39;t think he was saying the entire paper was &quot;utter rubbish&quot; - just the statement that he highlighted. But I think this was really an aside, not the main point of his post. Regardless, it is incorrect to claim that Bob said the entire paper was &quot;utter rubbish&quot;, which is suggested by your last sentence.<br /><br />2) From my reading, the main topic of Bob&#39;s post was about the influence of errors in x-variables when looking for relationships between x and y. There are very few instances in ecology where errors in x variables are considered when conducting regressions. Sure, it has been known for some time. For example, from memory, Sokal and Rohlf mention the issue in their text Biometry when discussing Model II regression (my copy is not on my shelf so I can&#39;t check my memory of this). And I know it is considered in allometric studies. However, I cannot ever remember being questioned about this issue by reviewers or editors in any paper I have written. Bob was suggesting that is this an issue that should be considered more frequently. After all, it leads to biased estimates, and we have the technical skills to reduce that bias. Surely, as a discipline, we should employ techniques to correct known sources of bias?<br /><br />Cheers,<br /><br />Mick (http://mickresearch.wordpress.com/)</div>
</div>
</div>
