---
layout: post
title: "Optimal ecologist strategy"
date: 2011-08-02
comments: false
categories:
 - statistics
 - debate
 - BUGS
 - programming
 - JAGS
 - career
---

<div class='post'>
So I'd like to pick up with a thought from an earlier post.  The past 2 days I've had the pleasure of working down at the University of Florida with my boss <a href="http://domingo.zoology.ubc.ca/AvilesLab/labfront.html" target="_blank">Leticia Aviles</a> and our collaborator <a href="http://people.biology.ufl.edu/josemi/" target="_blank">Jose Miguel Ponciano</a>.  Working with Jose Miguel made me think about a question posed by Caroline Tucker and Marc Cadotte over at the <a href="http://evol-eco.blogspot.com/2011/07/empirical-divide.html" target="_blank">EEB and Flow blog</a>:  what is the optimal strategy to be an ecologist?   The underlying dichotomy posed by Likens and Lindenmayer is between empirical ecologists and the 3M's of modeling, meta-analysis and data mining.  My original thinking was an optimal strategy is what Tucker and Cadotte refer as the "glib answer"; to become less specialized and more generalized.   They suggest that instead ecologists continue to specialize and focus on collaboration between specialized experts.  While I am actually part of such a collaboration at the moment, I still find their answer unsatisfactory, and here's why.<br /><br />To begin with I'll abandon the notion that the divide between the field ecologist and a 3M ecologist is one of empiricism vs theory.  I agree with one astute commenter on the original EEB and Flow blog post who noted that a 3M person is more often someone undertaking a statistical exercise than a true theoretical one.  So now lets assume the division is more between the field ecologist and what I think of as a statistical/mathematical gymnast.  As modern ecologists though I believe we need to gain a larger and larger mathematical and statistical skill set and still be good field/ empirical people.  This is because we don't live in what I think of as an "ANOVA world" anymore (thats not a dig about ANOVA in any way though, I just think of it as a very common and typical statistical test) .    The analytical tools available to us now are far more advanced than say 20 years ago (I'm imagining this in part because I was only 12 20 years ago).  Its no longer sufficient to take some data and plug it into JMP.  Let me illustrate with an example that is both trivial and complicated.  Imagine a hierarchical model normal model with j groups and i observations.<br /><br /><a href="http://www.codecogs.com/eqnedit.php?latex=y_{i}\sim N(\theta _{j}, \sigma_y )" target="_blank"><img src="http://latex.codecogs.com/gif.latex?y_{i}\sim N(\theta _{j}, \sigma_y )" title="y_{i}\sim N(\theta _{j}, \sigma_y )" /></a><br /><br /><a href="http://www.codecogs.com/eqnedit.php?latex=\theta _{j}\sim N(\mu ,\sigma_{\theta})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\theta _{j}\sim N(\mu ,\sigma_{\theta})" title="\theta _{j}\sim N(\mu ,\sigma_{\theta})" /></a><br /><br /><br />I can easily fit this simple hierarchical model in JAGS (or WinBUGS for windows users) with the following BUGS code:<br /><br /><pre class="brush: csharp"><br />model {<br />#fit the data model<br /> for (i in 1:N) {<br />  y[i] ~ dnorm(y.hat[i], tau)<br />  y.hat[i] &lt;- mu[groups[i]] <br />  }<br />#model the hierarchical mean and variance<br /><br /> for(i in 1:j){<br />  mu[i] ~ dnorm(mu.a,tau.a)<br />  }<br />#Set priors for data model<br /> tau &lt;- pow(sigma, -2)<br /> sigma ~ dunif(0, 100)<br />#Set priors for hierarchical model<br />    tau.a ~ dgamma(.1,.001)<br /> sigma.a &lt;- 1/sqrt(tau.a)<br /> mu.a ~ dnorm(0, .0001)<br />}<br /></pre><br /><br />Next we can call this model from R using the package rjags, and then analyze it with coda.<br /><br /><br /><pre class="brush: csharp"><br /><br />library('rjags')<br />library('coda')<br />#Number of groups J<br />J &lt;- 6<br />#Number of observations per group K<br />K &lt;- 5<br />#Total number of observatinos<br />N &lt;- J*K<br />#Vector of hierarchical means<br />mus &lt;- rnorm(J, 50, 25)<br />#generate a data vector<br />my.data &lt;- vector()<br />my.groups &lt;- vector()<br />for(i in 1:J){<br />  my.data &lt;- c(my.data,rnorm(K,mus[i],5))<br />  my.groups &lt;- c(my.groups,rep(i,K))<br />}<br />hierEx.mod &lt;- jags.model('simpmean.bug',data=list("y"=my.data,"groups"=my.groups,"N"=N,"j"=J),n.chains=3)<br />update(hierEx.mod,10000)<br />hierExsamp &lt;-coda.samples(hierEx.mod,c('mu.a','sigma.a','mu','sigma'),n.iter=1000,thin=10)<br />summary(hierExsamp)<br />plot(hierExsamp)<br /><br /></pre><br /><br />This output gives:<br /><div class="nobr"><table> <tr><td>Parameter</td>  <td>Bayesian Estimate</td>  <td>Simulated Value</td>  <td>True value</td> </tr> <tr> <td>theta[1]</td>  <td>63.5 ( 59.5 , 66.9 )</td>  <td>63.3</td>  <td>60.1</td> </tr> <tr>  <td>theta[2]</td>  <td>73.8 ( 70.2 , 78 )</td>  <td>73.8</td>  <td>71.4</td> </tr> <tr>  <td>theta[3]</td>  <td>104 ( 100.3 , 107.6 )</td>  <td>104.5</td>  <td>104.1</td> </tr> <tr>  <td>theta[4]</td>  <td>61.8 ( 57.7 , 65.7 )</td>  <td>62</td>  <td>58.4</td> </tr> <tr>  <td>theta[5]</td>  <td>64.4 ( 60.5 , 68.5 )</td>  <td>64.3</td>  <td>62.9</td> </tr> <tr>  <td>theta[6]</td>  <td>4.9 ( 0.9 , 8.9 )</td>  <td>4.7</td>  <td>8.3</td> </tr> <tr>  <td>sigma_y</td>  <td>4.3 ( 3.3 , 5.9 )</td>  <td>3.9</td>  <td>5</td> </tr> <tr>  <td>mu</td>  <td>59.3 ( 23.8 , 88 )</td>  <td>60.8</td>  <td>50</td> </tr> <tr>  <td>sigma_theta</td>  <td>33.6 ( 18.5 , 79.7 )</td>  <td>30.4</td>  <td>25</td> </tr></table></div>It should be obvious that the hierarchical estimates (with credible intervals) are very close to both the true values and the actual expected values calculated from the simulated data.  This is of course a very trivial problem that's so simple its really only useful for illustrative purposes.  But this whole techniques relies on a relatively new and non-trivial set of tools such as R, BUGS, as well as all the advancements in the math behind Bayesian statistics and MCMC algorithms.   If you're an ecologist who wants to take advantage of these tools, you'd need a working knowledge of probability theory, as well as programming skills in two scripting languages, R and BUGS.  Can you analyze data with traditional methods and canned software?  Sure, and I would never say that an elegant experiment is ever diminished by simple statistics, and advanced statistics and modeling don't make a bad experiment good.  But that said, good experiments analyzed with canned software are limited by the assumptions of the procedures that are available.  I think in 2011 ecologists need to be more general because of the advances in computational methods.  Skills like programming are needed for stochastic simulation, and a good understanding of calculus and probability theory are needed to make use of state-space and other hierarchical models.  I would argue that these skills are necessary to enable collaborations with other specialists.<br /><br />I had a great example of that these past few days working with Jose Miguel.  He helped us create very complicated likelihood functions for our spider metapopulation data with math that was way beyond me, but its up to me to take these likelihoods and fit them using MCMC methods. This collaboration seems like the sort of thing that Tucker and Cadotte were suggesting is fruitful among specialists, but I would argue I'm far more of a generalist and that collaboration works better because of that generalism.   I think its that generalism that will allow for great collaboration, so while I might not be able to write my own genetic algorithms, I do know enough about them and programming that I think I could collaborate with a computer scientist on an ecological problem using them.   About two years ago I observed the inverse as a part of a working group using modern computation methods (ANN's, GA's, classifier algorithms etc..) on a long term plankton data set.  The problem was that the group was headed by a senior ecologist who was very steeped in empirical methods and natural history, and otherwise consisted of engineers.  The knowledge gap was so big that despite the best intentions not much really came from it. <br /><br />So what is the best approach for a younger ecologist looking to have a successful career?  I would argue its still being a generalist where you are always trying keep up with the statistical and computational advances in the field (although I'm not successful yet so maybe this isn't the best idea).  Working with Jose Miguel brought the limitations of my own generalism into sharp relief, but also made me realize that as long as generalists don't ever think they can do it all, there is a strength in being to work with people from a variety other disciplines.</div>
